{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alex-smith-uwec/NLP_Spring2025/blob/main/Starter_Medical_Questions_Assignment_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28868e6f",
      "metadata": {
        "id": "28868e6f"
      },
      "source": [
        "#  NLP Assignment: Fine-Tuning a Transformer on Medical Question Pairs\n",
        "\n",
        "In this assignment, you will fine-tune a transformer model to classify whether pairs of medical questions are paraphrases of each other.\n",
        "\n",
        "Once you have everyting in place and before training, you should restart and change the runtime to TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b638748",
      "metadata": {
        "id": "3b638748"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##TODO: set random seed to your Blugold ID\n",
        "seed=##\n",
        "##Enter your name here:"
      ],
      "metadata": {
        "id": "ajIUUTKzbh88"
      },
      "id": "ajIUUTKzbh88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "93724b3f",
      "metadata": {
        "id": "93724b3f"
      },
      "source": [
        "## Step 1: Load the Dataset\n",
        "Use the `datasets` library to load the [curaihealth medical_questions_pairs](https://huggingface.co/datasets/curaihealth/medical_questions_pairs) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2c2c7f",
      "metadata": {
        "id": "0b2c2c7f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# TODO: Load the dataset\n",
        "\n",
        "dataset = ##\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b825eb7",
      "metadata": {
        "id": "7b825eb7"
      },
      "source": [
        "##  Train/Validation/Test Split\n",
        "The `medical_questions_pairs` dataset only provides a single training set. You need to create your own train, validation, and test sets.\n",
        "\n",
        "We'll split the dataset into:\n",
        "- **Train:** 80%\n",
        "- **Validation:** 10%\n",
        "- **Test:** 10%\n",
        "\n",
        "Use `train_test_split` from the `datasets` library to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1d578e",
      "metadata": {
        "id": "fc1d578e"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Step 1: Split into train + temp (val + test)\n",
        "temp_split = dataset['train'].train_test_split(test_size=0.2, seed=seed)\n",
        "\n",
        "# Step 2: Split temp into validation + test (50/50 of temp = 10% each)\n",
        "val_test_split = temp_split['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "\n",
        "# Step 3: Combine splits into a DatasetDict\n",
        "split_dataset = DatasetDict({\n",
        "    'train': temp_split['train'],\n",
        "    'validation': val_test_split['train'],\n",
        "    'test': val_test_split['test']\n",
        "})\n",
        "\n",
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: find an index  so that the corresponding validation question pair has label 0\n",
        "idx_0=##\n",
        "split_dataset['validation'][idx_0]\n"
      ],
      "metadata": {
        "id": "2fftS7AGX5PC"
      },
      "id": "2fftS7AGX5PC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: find an index so that the corresponding validation question pair has label 1\n",
        "idx_1=##\n",
        "split_dataset['validation'][idx_1]"
      ],
      "metadata": {
        "id": "_4lgIlTpZJR2"
      },
      "id": "_4lgIlTpZJR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d57071c0",
      "metadata": {
        "id": "d57071c0"
      },
      "source": [
        "## Step 2: Explore and Preprocess\n",
        "Examine the fields. Tokenize question pairs using a pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ffad30",
      "metadata": {
        "id": "f4ffad30"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        " ##Choose a model checkpoint\n",
        "checkpoint = 'microsoft/MiniLM-L12-H384-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "## Tokenization function\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example['question_1'], example['question_2'], truncation=True, padding='max_length',max_length=256)\n",
        "\n",
        "##Apply to dataset\n",
        "tokenized = split_dataset.map(tokenize_fn, batched=True)\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized['train'][0])"
      ],
      "metadata": {
        "id": "Sutjbr3J2C_z"
      },
      "id": "Sutjbr3J2C_z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "eb087f5b",
      "metadata": {
        "id": "eb087f5b"
      },
      "source": [
        "## Step 3: Load Model\n",
        "Load a model for sequence classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a74fb3",
      "metadata": {
        "id": "54a74fb3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# TODO: Define the model with correct number of labels\n",
        "model = ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3099290f",
      "metadata": {
        "id": "3099290f"
      },
      "source": [
        "## Step 4: Define Training Arguments\n",
        "Use Hugging Face `TrainingArguments` to configure training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713acd6c",
      "metadata": {
        "id": "713acd6c"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985e83eb",
      "metadata": {
        "id": "985e83eb"
      },
      "source": [
        "## Step 5: Define Trainer\n",
        "Set up the `Trainer` object and begin training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e71ffb0",
      "metadata": {
        "id": "9e71ffb0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00c4b79",
      "metadata": {
        "id": "c00c4b79"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "##Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNXLYwZXM7Ub",
      "metadata": {
        "id": "hNXLYwZXM7Ub"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20696227",
      "metadata": {
        "id": "20696227"
      },
      "source": [
        "## Step 6: Evaluation\n",
        "Evaluate and inspect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851ff034",
      "metadata": {
        "id": "851ff034"
      },
      "outputs": [],
      "source": [
        "##Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b573880c",
      "metadata": {
        "id": "b573880c"
      },
      "source": [
        "## Training Accuracy\n",
        "Now that training is complete, let's evaluate the model on the training set to report training accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02493441",
      "metadata": {
        "id": "02493441"
      },
      "outputs": [],
      "source": [
        "# Evaluate on training data\n",
        "train_metrics = trainer.evaluate(tokenized[\"train\"])\n",
        "print(f\"Training Accuracy: {train_metrics['eval_accuracy']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}